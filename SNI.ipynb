{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "#import argparse\n",
    "import numpy as np\n",
    "import jieba\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_json(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def write_data_json(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "        \n",
    "def read_json_line(filename):\n",
    "    data_list = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data_list.append(json.loads(line))\n",
    "    return data_list\n",
    "\n",
    "def read_math23k_json(filename):\n",
    "    data_list = []\n",
    "    with open(filename, 'r') as f:\n",
    "        count = 0\n",
    "        string = ''\n",
    "        for line in f:\n",
    "            count += 1\n",
    "            string += line\n",
    "            if count % 7 == 0:\n",
    "                #print string\n",
    "                data_list.append(json.loads(string))\n",
    "                string = ''\n",
    "    #print data_list[-1]\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "math23k_test = read_math23k_json(\"./data/math23k_test.json\")\n",
    "math23k_train = read_math23k_json(\"./data/math23k_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.895 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "for elem in math23k_train:\n",
    "    origin = elem['original_text']\n",
    "    origin_text = ' '.join(jieba.cut(origin, cut_all=False))\n",
    "    elem['new_split'] = origin_text\n",
    "for elem in math23k_test:\n",
    "    origin = elem['original_text']\n",
    "    origin_text = ' '.join(jieba.cut(origin, cut_all=False))\n",
    "    elem['new_split'] = origin_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_number(text_list):\n",
    "    new_list = []\n",
    "    i = 0\n",
    "    while i<len(text_list):\n",
    "        if text_list[i] == '(' and i+4<len(text_list) and text_list[i+4] == ')':\n",
    "            sub = ''.join(text_list[i:i+5])\n",
    "            new_list.append(sub)\n",
    "            i = i+5\n",
    "        else:\n",
    "            new_list.append(text_list[i])\n",
    "            i += 1\n",
    "    #text_list = new_list[:]\n",
    "    #new_list = []\n",
    "    '''\n",
    "    i = 0\n",
    "    while i < len(text_list):\n",
    "        if '%' in text_list[i] and len(text_list[i])>1:\n",
    "            new_list.append(text_list[i])\n",
    "            new_list.append(text_list[i][-1])\n",
    "        else:\n",
    "            new_list.append(text_list[i])\n",
    "        i += 1\n",
    "    '''\n",
    "    return new_list        \n",
    "\n",
    "def is_number(word):\n",
    "    if word[0] == '(' and word[-1] == ')':\n",
    "        for elem_char in word:\n",
    "            if (elem_char.isdigit()):\n",
    "                return True\n",
    "        return False\n",
    "    if '(' in word and ')' in word and '/' in word and not word[-1].isdigit():\n",
    "        for elem_char in word:\n",
    "            if (elem_char.isdigit()):\n",
    "                return True\n",
    "        return False\n",
    "        #return True\n",
    "    if word[-1] == '%' and len(word)>1:\n",
    "        return True\n",
    "    if word[0].isdigit():\n",
    "        return True\n",
    "    if word[-1].isdigit():\n",
    "        return True\n",
    "    try:\n",
    "        float(word)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def split_num_and_unit(word):\n",
    "    num = ''\n",
    "    unit = ''\n",
    "    for idx in range(len(word)):                                                                                                                                                                                                                                              \n",
    "        char = word[idx]\n",
    "        if char.isdigit() or char in ['.', '/', '(', ')']:\n",
    "            num += char\n",
    "        else:\n",
    "            unit += char\n",
    "    return num, unit#.encode('utf-8')\n",
    "\n",
    "def mask_num(seg_text_list, equ_str):\n",
    "    origin_equ_str = equ_str[:]\n",
    "\n",
    "    alphas = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    num_list  = []\n",
    "    mask_seg_text = []\n",
    "    count = 0 \n",
    "    number_pos = []\n",
    "    num_temp_list = []\n",
    "    for w_i in range(len(seg_text_list)):\n",
    "        word = seg_text_list[w_i]\n",
    "        if word == '':\n",
    "            continue\n",
    "        if is_number(word):\n",
    "            mask_seg_text.append(\"temp_\"+alphas[count])\n",
    "            if '%' in word:\n",
    "                mask_seg_text.append('%')\n",
    "            #elif 'm' in word.lower() or 'g' in word.lower() or 'd' in word.lower():\n",
    "            elif len(set(alphas)&set(word.lower()))>0:\n",
    "                num, unit = split_num_and_unit(word)\n",
    "                mask_seg_text.append(unit)\n",
    "                word = num\n",
    "                \n",
    "            num_list.append(word)\n",
    "            number_pos.append(w_i)\n",
    "            num_temp_list.append(\"temp_\"+alphas[count])\n",
    "            count += 1\n",
    "        else:\n",
    "            mask_seg_text.append(word)\n",
    "    mask_equ_list = []\n",
    "    s_n = sorted([(w,i) for i,w in enumerate(num_list)], key=lambda x: len(str(x[0])), reverse=True)\n",
    "    if '3.14%' not in equ_str and '3.1416' not in equ_str:\n",
    "        equ_str = equ_str.replace('3.14', '&PI&', 15)\n",
    "    new_equ_str = ''\n",
    "    #print (s_n)\n",
    "    #print (equ_str)\n",
    "    for num, idx in s_n:\n",
    "        #num = num_list[idx]\n",
    "        equ_str = equ_str.replace(num, '&temp_'+alphas[idx]+'&', 15)\n",
    "        #if \n",
    "    #print (equ_str)\n",
    "        \n",
    "        \n",
    "    equ_list = []\n",
    "    num_set = ['0','1','2','3','4','5','6','7','8','9','%', '.']\n",
    "    for elem in equ_str.split('&'):\n",
    "        if 'temp' in elem or 'PI' in elem:\n",
    "            equ_list.append(elem)\n",
    "        else:\n",
    "            start = ''\n",
    "            for char in elem:\n",
    "                if char not in num_set:\n",
    "                    if start != '':\n",
    "                        equ_list.append(start)\n",
    "                    equ_list.append(char)\n",
    "                    start = ''\n",
    "                else:\n",
    "                    start += char\n",
    "            if start != '':\n",
    "                equ_list.append(start)\n",
    "    #rint (equ_list)\n",
    "    #rint ()\n",
    "    #reverse_equ_list = equ_list[::-1]\n",
    "    #reverse_equ_list.append('END_token')\n",
    "    #equ_list.append('END_token')\n",
    "    '''\n",
    "    print (' '.join(seg_text_list))\n",
    "    print (' '.join(mask_seg_text))\n",
    "    print (num_list)\n",
    "    print (' '.join(equ_list))\n",
    "    print (origin_equ_str)\n",
    "    print ()\n",
    "    '''\n",
    "    new_equ_list = []\n",
    "    for elem_equ in equ_list:\n",
    "        if elem_equ == '[':\n",
    "            elem_equ = '('\n",
    "        if elem_equ == ']':\n",
    "            elem_equ = ')'\n",
    "        new_equ_list.append(elem_equ)\n",
    "    equ_list = new_equ_list[:]\n",
    "    return mask_seg_text, num_list, equ_list, number_pos, num_temp_list\n",
    "\n",
    "\n",
    "\n",
    "def num_list_processed(num_list):\n",
    "    new_num_list = []\n",
    "    for num in num_list:\n",
    "        if '%' in num:\n",
    "            new_num_list.append(float(num[:-1])*1.0/100)\n",
    "        elif '(' in num:\n",
    "            new_num_list.append(eval(num))\n",
    "         \n",
    "        else:\n",
    "            num_,_ = split_num_and_unit(num)\n",
    "            new_num_list.append(float(num_))\n",
    "    return new_num_list\n",
    "\n",
    "def postfix_equation(equ_list):\n",
    "    stack = []\n",
    "    post_equ = []\n",
    "    op_list = ['+', '-', '*', '/', '^']\n",
    "    priori = {'^':3, '*':2, '/':2, '+':1, '-':1}\n",
    "    for elem in equ_list:\n",
    "        if elem == '(':\n",
    "            stack.append('(')\n",
    "        elif elem == ')':\n",
    "            while 1:\n",
    "                op = stack.pop()\n",
    "                if op == '(':\n",
    "                    break\n",
    "                else:\n",
    "                    post_equ.append(op)\n",
    "        elif elem in op_list:\n",
    "            while 1:\n",
    "                if stack == []:\n",
    "                    break\n",
    "                elif stack[-1] == '(':\n",
    "                    break\n",
    "                elif priori[elem] > priori[stack[-1]]:\n",
    "                    break\n",
    "                else:\n",
    "                    op = stack.pop()\n",
    "                    post_equ.append(op)\n",
    "            stack.append(elem)\n",
    "        else:\n",
    "            #if elem == 'PI':\n",
    "            #    post_equ.append('3.14')\n",
    "            #else:\n",
    "            #    post_equ.append(elem)\n",
    "            post_equ.append(elem)\n",
    "    while stack != []:\n",
    "        post_equ.append(stack.pop())\n",
    "    return post_equ\n",
    "\n",
    "def post_solver(post_equ):\n",
    "    stack = []\n",
    "    op_list = ['+', '-', '/', '*', '^']\n",
    "    for elem in post_equ:\n",
    "        if elem not in op_list:\n",
    "            op_v = elem\n",
    "            #if '%' in op_v:\n",
    "            #    op_v = float(op_v[:-1])/100.0\n",
    "            stack.append(str(op_v))\n",
    "        elif elem in op_list:\n",
    "            op_v_1 = stack.pop()\n",
    "            op_v_1 = float(op_v_1)\n",
    "            op_v_2 = stack.pop()\n",
    "            op_v_2 = float(op_v_2)\n",
    "            if elem == '+':\n",
    "                stack.append(str(op_v_2+op_v_1))\n",
    "            elif elem == '-':\n",
    "                stack.append(str(op_v_2-op_v_1))\n",
    "            elif elem == '*':\n",
    "                stack.append(str(op_v_2*op_v_1))\n",
    "            elif elem == '/':\n",
    "                stack.append(str(op_v_2/op_v_1))\n",
    "            else:\n",
    "                stack.append(str(op_v_2**op_v_1))\n",
    "    return stack.pop()\n",
    "         \n",
    "def solve_equation(equ_list):\n",
    "    if '=' in equ_list:\n",
    "        equ_list = equ_list[2:]\n",
    "   \n",
    "    post_equ = postfix_equation(equ_list)\n",
    "    ans = post_solver(post_equ)\n",
    "    return ans\n",
    "\n",
    "def inverse_temp_to_num(equ_list, num_list):\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    new_equ_list = []\n",
    "    for elem in equ_list:\n",
    "        if 'temp' in elem:\n",
    "            index = alphabet.index(elem[-1])\n",
    "            new_equ_list.append(str(num_list[index]))\n",
    "        elif 'PI' == elem:\n",
    "            new_equ_list.append('3.14')\n",
    "        else:\n",
    "            new_equ_list.append(elem)\n",
    "    return new_equ_list\n",
    "\n",
    "def ans_num_joint(word):\n",
    "    i = 0\n",
    "    new = []\n",
    "    str_ = ''\n",
    "    while i<len(word):\n",
    "        if word[i].isdigit() or word[i] in ['.','-']:\n",
    "            str_ += word[i]\n",
    "        else:\n",
    "            if str_ != '':\n",
    "                new.append(str_)\n",
    "                str_ = ''\n",
    "            new.append(word[i])\n",
    "        i+=1\n",
    "    return solve_equation(new)\n",
    "\n",
    "def ans_decimal_exception(word):\n",
    "    word = str(word)\n",
    "    ind = word.find('(')\n",
    "    word = word[:ind]+'+'+word[ind:]\n",
    "    return ans_num_joint(word)\n",
    "\n",
    "def ans_process(word):\n",
    "    try:\n",
    "        float(word)\n",
    "        return float(word)\n",
    "    except:\n",
    "        if '%' in str(word):\n",
    "            return float(word[:-1])/100\n",
    "        if str(word)[0]=='(' and str(word)[-1]==')':\n",
    "            return ans_num_joint(word)\n",
    "        if str(word)[0] != '(' and str(word)[-1]==')':\n",
    "            return ans_decimal_exception(word)\n",
    "    return -float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in math23k_train:\n",
    "    pid = elem['id']\n",
    "    split_text = elem['new_split']\n",
    "    text_list = split_text.split(' ')\n",
    "    join_text_list = joint_number(text_list) # '(' '1' '/' '5' ')' -> '(1/5)'\n",
    "    equation_str = elem['equation']\n",
    "    mask_seg_text, num_list, temp_equ_list, num_pos, num_temp_list = mask_num(join_text_list, equation_str)\n",
    "    #print (mask_seg_text)\n",
    "    #print (split_text)\n",
    "    #print (num_list)\n",
    "    #print ([mask_seg_text[pos] for pos in num_pos] )\n",
    "    #print (num_pos)\n",
    "    elem['mask_seg_text'] = mask_seg_text\n",
    "    context_list = []\n",
    "    for pos in num_pos:\n",
    "        if pos - 3<0:\n",
    "            left = 0\n",
    "        else:\n",
    "            left = pos - 3\n",
    "        if pos + 4 >= len(mask_seg_text):\n",
    "            right = len(mask_seg_text)\n",
    "        else:\n",
    "            right = pos + 4\n",
    "        context_list.append(mask_seg_text[left: right])\n",
    "    elem['context_list'] = context_list[:]    \n",
    "    \n",
    "    num_labels = []\n",
    "    for temp_num in num_temp_list:\n",
    "        if temp_num not in temp_equ_list:\n",
    "            num_labels.append(0)\n",
    "        else:\n",
    "            num_labels.append(1)\n",
    "    elem['num_labels'] = num_labels\n",
    "    assert len(num_labels) == len(context_list), \"alignment error\"\n",
    "    #if 0 in num_labels:\n",
    "        #pass\n",
    "        #print (num_labels)\n",
    "        #print (num_temp_list)\n",
    "        #print (elem['context_list'])\n",
    "        #print (temp_equ_list)\n",
    "        #print ()\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in math23k_test:\n",
    "    pid = elem['id']\n",
    "    split_text = elem['new_split']\n",
    "    text_list = split_text.split(' ')\n",
    "    join_text_list = joint_number(text_list) # '(' '1' '/' '5' ')' -> '(1/5)'\n",
    "    equation_str = elem['equation']\n",
    "    mask_seg_text, num_list, temp_equ_list, num_pos, num_temp_list = mask_num(join_text_list, equation_str)\n",
    "    #print (mask_seg_text)\n",
    "    #print (split_text)\n",
    "    #print (num_list)\n",
    "    #print ([mask_seg_text[pos] for pos in num_pos] )\n",
    "    #print (num_pos)\n",
    "    elem['mask_seg_text'] = mask_seg_text\n",
    "    context_list = []\n",
    "    for pos in num_pos:\n",
    "        if pos - 3<0:\n",
    "            left = 0\n",
    "        else:\n",
    "            left = pos - 3\n",
    "        if pos + 4 >= len(mask_seg_text):\n",
    "            right = len(mask_seg_text)\n",
    "        else:\n",
    "            right = pos + 4\n",
    "        context_list.append(mask_seg_text[left: right])\n",
    "    elem['context_list'] = context_list[:]    \n",
    "    \n",
    "    num_labels = []\n",
    "    for temp_num in num_temp_list:\n",
    "        if temp_num not in temp_equ_list:\n",
    "            num_labels.append(0)\n",
    "        else:\n",
    "            num_labels.append(1)\n",
    "    elem['num_labels'] = num_labels\n",
    "    assert len(num_labels) == len(context_list), \"alignment error\"\n",
    "    #if 0 in num_labels:\n",
    "        #pass\n",
    "        #print (num_labels)\n",
    "        #print (num_temp_list)\n",
    "        #print (elem['context_list'])\n",
    "        #print (temp_equ_list)\n",
    "        #print ()\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_2_idx_sen(sen,  vocab_dict):                                                                                             \n",
    "    new = []\n",
    "    for word in sen:\n",
    "        if word not in vocab_dict:\n",
    "             new.append(vocab_dict['UNK_token'])\n",
    "        else:\n",
    "             new.append(vocab_dict[word])\n",
    "    return new\n",
    "\n",
    "def pad_sen(sen_idx_list, max_len=7, pad_idx=0):                                                                                                                                                                                                                            \n",
    "    return [pad_idx]*(max_len-len(sen_idx_list))+ sen_idx_list\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, math23k_train, math23k_test):\n",
    "        self.math23k_train_list = math23k_train\n",
    "        self.math23k_test_list = math23k_test\n",
    "        emb_dim = 128\n",
    "        self.emb_vectors, self.vocab_list = self.preprocess_and_word2vec(emb_dim)\n",
    "        self.vocab_dict = dict([(elem, idx) for idx, elem in enumerate(self.vocab_list)])\n",
    "        self.vocab_len = len(self.vocab_list)\n",
    "\n",
    "        self.classes_len = 2\n",
    "        \n",
    "    def preprocess_and_word2vec(self, emb_dim):\n",
    "        new_data ={}\n",
    "        sentences = []\n",
    "        for elem in self.math23k_train_list:\n",
    "            sentence = elem['new_split'].strip().split(' ')\n",
    "            sentences.append(sentence)\n",
    "            for elem in sentence:\n",
    "                new_data[elem] = new_data.get(elem, 0) + 1\n",
    "        \n",
    "        from gensim.models import word2vec\n",
    "        model = word2vec.Word2Vec(sentences, size=emb_dim, min_count=1)\n",
    "\n",
    "        token_list = ['PAD_token', 'UNK_token']\n",
    "        emb_vectors = []\n",
    "        emb_vectors.append(np.zeros((emb_dim)))\n",
    "        emb_vectors.append(np.random.rand((emb_dim))/1000.0)\n",
    "        \n",
    "        for k, v in new_data.items(): \n",
    "            token_list.append(k)\n",
    "            emb_vectors.append(np.array(model.wv[k]))\n",
    "        \n",
    "        emb_vectors = np.array(emb_vectors)\n",
    "        return emb_vectors, token_list\n",
    "\n",
    "    def get_batch_train(self, data_set, batch_size):\n",
    "        data_list = []\n",
    "        label_list = []\n",
    "        id_list = []\n",
    "        for elem in data_set:\n",
    "            context_list = elem['context_list'] \n",
    "            num_labels = elem['num_labels']\n",
    "            id_ = elem['id']\n",
    "            for c_i in range(len(context_list)):\n",
    "                context = context_list[c_i]\n",
    "                data_list.append(context)\n",
    "                id_list.append([id_, c_i])\n",
    "            for num in num_labels:\n",
    "                label_list.append(num)\n",
    "             \n",
    "        data_set = list(zip(data_list, label_list, id_list))\n",
    "        #print (type(data_set))\n",
    "        random.seed(10)\n",
    "        random.shuffle(data_set)\n",
    "        \n",
    "        batch_num = int(len(data_set)/batch_size)+1\n",
    "        for idx in range(batch_num):\n",
    "            batch_start = idx*batch_size\n",
    "            batch_end = min((idx+1)*batch_size, len(data_set))\n",
    "            \n",
    "            data_batch = data_set[batch_start: batch_end]\n",
    "            batch_x, batch_y, batch_ids = zip(*data_batch)\n",
    "            \n",
    "            batch_encode_idx = []\n",
    "            batch_encode_len = []\n",
    "            batch_encode_pad_idx = []\n",
    "            \n",
    "            for se_i in range(len(batch_x)):\n",
    "                encode_sen = batch_x[se_i] # splitted\n",
    "                encode_sen_idx = string_2_idx_sen(encode_sen, self.vocab_dict)\n",
    "                encode_sen_pad_idx = pad_sen(encode_sen_idx, 7, self.vocab_dict['PAD_token']) \n",
    "                batch_encode_idx.append(encode_sen_idx)\n",
    "                batch_encode_len.append(len(encode_sen_idx))\n",
    "                batch_encode_pad_idx.append(encode_sen_pad_idx)\n",
    "            \n",
    "            batch_data_dict = dict()\n",
    "            batch_data_dict['batch_encode_idx'] = batch_encode_idx\n",
    "            batch_data_dict['batch_encode_pad_idx'] = batch_encode_pad_idx\n",
    "            batch_data_dict['batch_encode_len'] = batch_encode_len \n",
    "            batch_data_dict['batch_y'] = batch_y\n",
    "            batch_data_dict['batch_x'] = batch_x\n",
    "            batch_data_dict['batch_ids'] = batch_ids\n",
    "            \n",
    "            new_batch_data_dict = self._sorted_batch(batch_data_dict)\n",
    "            yield new_batch_data_dict#new_batch_data_dict\n",
    "\n",
    "    def _sorted_batch(self, batch_data_dict):\n",
    "        new_batch_data_dict = dict()\n",
    "        batch_encode_len = np.array(batch_data_dict['batch_encode_len'])\n",
    "        sort_idx = np.argsort(-batch_encode_len)\n",
    "        for key, value in batch_data_dict.items():\n",
    "            new_batch_data_dict[key] = np.array(value)[sort_idx]\n",
    "        return new_batch_data_dict\n",
    "    \n",
    "            \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_loader = DataLoader(math23k_train, math23k_test)\n",
    "#train_generator = data_loader.get_batch_train(32)\n",
    "#for elem in train_generator:\n",
    "    #print (elem)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(math23k_train, math23k_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, input_dropout_p, dropout_p, \\\n",
    "                          n_layers, rnn_cell_name):\n",
    "        super(BaseRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.input_dropout_p = input_dropout_p\n",
    "        self.input_dropout = nn.Dropout(p=input_dropout_p)\n",
    "        self.rnn_cell_name = rnn_cell_name\n",
    "        if rnn_cell_name.lower() == 'lstm':\n",
    "            self.rnn_cell = nn.LSTM\n",
    "        elif rnn_cell_name.lower() == 'gru':\n",
    "            self.rnn_cell = nn.GRU\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported RNN Cell: {0}\".format(rnn_cell_name))\n",
    "        self.dropout_p = dropout_p\n",
    " \n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class EncoderRNN(BaseRNN):\n",
    "    def __init__(self, vocab_size, embed_model, emb_size=100, hidden_size=128, \\\n",
    "                 input_dropout_p=0, dropout_p=0, n_layers=1, bidirectional=False, \\\n",
    "                 rnn_cell=None, rnn_cell_name='gru', variable_lengths_flag=True):\n",
    "        super(EncoderRNN, self).__init__(vocab_size, emb_size, hidden_size,\n",
    "              input_dropout_p, dropout_p, n_layers, rnn_cell_name)\n",
    "        self.variable_lengths_flag = variable_lengths_flag\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding = embed_model\n",
    "        if rnn_cell == None:\n",
    "            self.rnn = self.rnn_cell(emb_size, hidden_size, n_layers,\n",
    "                                 batch_first=True, bidirectional=bidirectional, dropout=dropout_p)\n",
    "        else:\n",
    "            self.rnn = rnn_cell\n",
    " \n",
    "    def forward(self, input_var, input_lengths=None):\n",
    "        embedded = self.embedding(input_var)\n",
    "        embedded = self.input_dropout(embedded)\n",
    "        #pdb.set_trace()\n",
    "        if self.variable_lengths_flag:\n",
    "            embedded = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, batch_first=True)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        if self.variable_lengths_flag:\n",
    "            output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        return output, hidden\n",
    "    \n",
    "    def no_sorted_forward(self, input_var, seq_lengths=None):\n",
    "        embedded = self.embedding(input_var)\n",
    "        inputs = self.input_dropout(embedded)\n",
    "        \n",
    "        sorted_seq_lengths, indices = torch.sort(seq_lengths, descending=True)\n",
    "        _, desorted_indices = torch.sort(indices, descending=False)\n",
    "        inputs = inputs[indices]\n",
    "        packed_inputs = nn.utils.rnn.pack_padded_sequence(inputs, sorted_seq_lengths.cpu().numpy(), batch_first=True)\n",
    "        res, state = self.rnn(packed_inputs)\n",
    "        padded_res, _ = nn.utils.rnn.pad_packed_sequence(res, batch_first=True)\n",
    "        desorted_res = padded_res[desorted_indices]\n",
    "        return desorted_res, state\n",
    "\n",
    "def _cat_directions(h, bidirectional):\n",
    "    if bidirectional:\n",
    "        h = torch.cat([h[0:h.size(0):2], h[1:h.size(0):2]], 2)\n",
    "    return h\n",
    "    \n",
    "class SNI_net(nn.Module):\n",
    "    def __init__(self, data_loader, encode_params):\n",
    "        super(SNI_net, self).__init__()\n",
    "        \n",
    "        self.embed_model =  nn.Embedding(data_loader.vocab_len, encode_params['emb_size'])\n",
    "        self.encoder = EncoderRNN(vocab_size = data_loader.vocab_len,\n",
    "                              embed_model = self.embed_model,\n",
    "                              emb_size = encode_params['emb_size'],\n",
    "                              hidden_size = encode_params['hidden_size'],\n",
    "                              input_dropout_p = encode_params['input_dropout_p'],\n",
    "                              dropout_p = encode_params['dropout_p'],\n",
    "                              n_layers = encode_params['n_layers'],\n",
    "                              bidirectional = encode_params['bidirectional'],\n",
    "                              rnn_cell = encode_params['rnn_cell'],\n",
    "                              rnn_cell_name = encode_params['rnn_cell_name'],\n",
    "                              variable_lengths_flag = encode_params['variable_lengths_flag'])\n",
    "        self.activation = F.relu\n",
    "        self.encode_params = encode_params\n",
    "        if encode_params['bidirectional'] == True:\n",
    "            fnn_size = encode_params['hidden_size']*2\n",
    "        else:\n",
    "            fnn_size = encode_params['hidden_size']\n",
    "        \n",
    "        self.numClasses = data_loader.classes_len \n",
    "        self.data_loader = data_loader\n",
    "        self.W = nn.Linear(fnn_size, fnn_size, bias=True)\n",
    "        self.projection = nn.Linear(fnn_size, self.numClasses, bias=True)\n",
    "        \n",
    "    def forward(self, input_tensor, input_lengths):\n",
    "        '''\n",
    "        1--: torch.Size([32, 7])\n",
    "        2--: torch.Size([2, 32, 128])\n",
    "        3--: torch.Size([1, 32, 256])\n",
    "        4--: torch.Size([1, 32, 2])\n",
    "        '''\n",
    "        #print ('1--:', input_tensor.size())\n",
    "        #encoder_outputs, encoder_hidden = self.encoder.no_sorted_forward(input_tensor, input_lengths)\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_tensor, input_lengths)\n",
    "        #print ('2--:', encoder_hidden[0].size())\n",
    "        encoder_hidden_ = _cat_directions(encoder_hidden[0], self.encode_params['bidirectional'])\n",
    "        #print ('3--:', encoder_hidden_.size())\n",
    "        fc_input = encoder_hidden_.squeeze(0)\n",
    "        currentNode = self.activation(self.W(fc_input))\n",
    "        proj_probs = self.projection(currentNode)\n",
    "        #print ('4--:', proj_probs.size()) \n",
    "        return proj_probs\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, data_loader, params, optimizer):\n",
    "        self.data_loader = data_loader\n",
    "        self.params = params\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def train(self, model):\n",
    "        for epoch in range(self.params['start_epoch'], self.params['n_epoch']):\n",
    "            train_generator = self.data_loader.get_batch_train(self.data_loader.math23k_train_list, self.params['batch_size'])\n",
    "            epoch_loss = 0.0#torch.FloatTensor([0])[0]#.cuda()\n",
    "            pred_count = 0\n",
    "            total = 0\n",
    "            for batch_elem in train_generator:\n",
    "                batch_y = batch_elem['batch_y']\n",
    "                batch_encode_pad_idx = batch_elem['batch_encode_pad_idx']\n",
    "                batch_encode_len = batch_elem['batch_encode_len']\n",
    "\n",
    "                batch_encode_pad_idx_tensor = torch.LongTensor(batch_encode_pad_idx).cuda()\n",
    "                batch_encode_len_tensor = torch.LongTensor(batch_encode_len).cuda()\n",
    "                batch_y_tensor = torch.LongTensor(batch_y).cuda()\n",
    "                \n",
    "                proj_probs = model(batch_encode_pad_idx_tensor, batch_encode_len_tensor)\n",
    "                #max_probs = proj_probs.max(dim=1)[1]\n",
    "                \n",
    "                loss = F.cross_entropy(input=proj_probs, target=batch_y_tensor)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                loss.backward()#(retain_graph=True)\n",
    "                clip_grad_norm_(model.parameters(), 5, norm_type=2.)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                batch_preds = proj_probs.max(dim=1)[1]\n",
    "                \n",
    "                pred_count += torch.sum((batch_preds == batch_y_tensor).int()).item()\n",
    "                total += len(batch_y)\n",
    "            print (\"epoch:\", epoch, 'loss:', epoch_loss*1.0/total, 'train_acc:', pred_count*1.0/total)\n",
    "             \n",
    "            train_generator = self.data_loader.get_batch_train(self.data_loader.math23k_train_list, self.params['batch_size'])\n",
    "            train_accumulate_dict = self.predict_mid(model, train_generator)\n",
    "            print (\"i_train_acc:\", self.predict_acc(self.data_loader.math23k_train_list, train_accumulate_dict))\n",
    "            test_generator = self.data_loader.get_batch_train(self.data_loader.math23k_test_list, self.params['batch_size'])\n",
    "            test_accumulate_dict = self.predict_mid(model, test_generator)\n",
    "            print (\"i_test_acc:\", self.predict_acc(self.data_loader.math23k_test_list, test_accumulate_dict))\n",
    "            print ()\n",
    "        \n",
    "    def predict_acc(self, data_list, accumulate_dict):\n",
    "        acc = 0.0\n",
    "        for elem in data_list:\n",
    "            context_list = elem['context_list'] \n",
    "            num_labels = elem['num_labels']\n",
    "            id_ = elem['id']\n",
    "            flag = 1\n",
    "            for c_i in range(len(context_list)):\n",
    "                context = context_list[c_i]\n",
    "                label_ = num_labels[c_i]\n",
    "                k_ = str(id_) + '_' + str(c_i)\n",
    "                if label_ != accumulate_dict[k_]:\n",
    "                    flag = 0\n",
    "            acc += flag\n",
    "        return acc*1.0/len(data_list)\n",
    "                \n",
    "            \n",
    "    \n",
    "    def predict_mid(self, model, data_generator):\n",
    "        accumulate_dict = {}\n",
    "        for batch_elem in data_generator:\n",
    "            batch_y = batch_elem['batch_y']\n",
    "            batch_encode_pad_idx = batch_elem['batch_encode_pad_idx']\n",
    "            batch_encode_len = batch_elem['batch_encode_len']\n",
    "\n",
    "            batch_encode_pad_idx_tensor = torch.LongTensor(batch_encode_pad_idx).cuda()\n",
    "            batch_encode_len_tensor = torch.LongTensor(batch_encode_len).cuda()\n",
    "            batch_y_tensor = torch.LongTensor(batch_y).cuda()\n",
    "\n",
    "            proj_probs = model(batch_encode_pad_idx_tensor, batch_encode_len_tensor)\n",
    "            \n",
    "            batch_preds = proj_probs.max(dim=1)[1]\n",
    "            \n",
    "            batch_ids = batch_elem['batch_ids']\n",
    "            \n",
    "            for b_i in range(len(batch_y)):\n",
    "                id_ = batch_ids[b_i]\n",
    "                p_ = batch_preds[b_i].item()\n",
    "                accumulate_dict['_'.join(id_)] = p_\n",
    "        return accumulate_dict\n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanglei/.pyvenvs/py35/lib/python3.5/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 0.005119848604746655 train_acc: 0.9544065165554435\n",
      "i_train_acc: 0.899557801642451\n",
      "i_test_acc: 0.875\n",
      "\n",
      "epoch: 2 loss: 0.004385098702973675 train_acc: 0.9580232688399187\n",
      "i_train_acc: 0.9072737117588665\n",
      "i_test_acc: 0.884\n",
      "\n",
      "epoch: 3 loss: 0.004034018811841637 train_acc: 0.9613999711940083\n",
      "i_train_acc: 0.9133200974641278\n",
      "i_test_acc: 0.89\n",
      "\n",
      "epoch: 4 loss: 0.0037771988537166775 train_acc: 0.9632883639797077\n",
      "i_train_acc: 0.9169750022561141\n",
      "i_test_acc: 0.893\n",
      "\n",
      "epoch: 5 loss: 0.0035903986722804664 train_acc: 0.964776673548098\n",
      "i_train_acc: 0.918779893511416\n",
      "i_test_acc: 0.891\n",
      "\n",
      "epoch: 6 loss: 0.003430682936739963 train_acc: 0.9659769232000256\n",
      "i_train_acc: 0.9204945402039527\n",
      "i_test_acc: 0.899\n",
      "\n",
      "epoch: 7 loss: 0.0032999439851993167 train_acc: 0.9667610863059516\n",
      "i_train_acc: 0.923246999368288\n",
      "i_test_acc: 0.904\n",
      "\n",
      "epoch: 8 loss: 0.0031652328617838496 train_acc: 0.9676732760414166\n",
      "i_train_acc: 0.9252323797491201\n",
      "i_test_acc: 0.898\n",
      "\n",
      "epoch: 9 loss: 0.0030259740918014825 train_acc: 0.9690335589802679\n",
      "i_train_acc: 0.9277141052251602\n",
      "i_test_acc: 0.901\n",
      "\n",
      "epoch: 10 loss: 0.0029162485336997742 train_acc: 0.9698497287435787\n",
      "i_train_acc: 0.9279848389134555\n",
      "i_test_acc: 0.897\n",
      "\n",
      "epoch: 11 loss: 0.0028134992957599978 train_acc: 0.9706819018355818\n",
      "i_train_acc: 0.9307824203591734\n",
      "i_test_acc: 0.895\n",
      "\n",
      "epoch: 12 loss: 0.002721316858149325 train_acc: 0.971258021668507\n",
      "i_train_acc: 0.9329934121469181\n",
      "i_test_acc: 0.899\n",
      "\n",
      "epoch: 13 loss: 0.0026075419207608106 train_acc: 0.9722502280474339\n",
      "i_train_acc: 0.9334446349607436\n",
      "i_test_acc: 0.899\n",
      "\n",
      "epoch: 14 loss: 0.0024885012567552912 train_acc: 0.9730663978107447\n",
      "i_train_acc: 0.936197094125079\n",
      "i_test_acc: 0.906\n",
      "\n",
      "epoch: 15 loss: 0.0023919802125476762 train_acc: 0.9737545409445164\n",
      "i_train_acc: 0.9380471076617634\n",
      "i_test_acc: 0.905\n",
      "\n",
      "epoch: 16 loss: 0.0022918607622804967 train_acc: 0.9748587706242898\n",
      "i_train_acc: 0.9396263875101525\n",
      "i_test_acc: 0.896\n",
      "\n",
      "epoch: 17 loss: 0.002201730477501233 train_acc: 0.975818970345832\n",
      "i_train_acc: 0.9404837108564209\n",
      "i_test_acc: 0.899\n",
      "\n",
      "epoch: 18 loss: 0.0021176717463792364 train_acc: 0.9764911101509114\n",
      "i_train_acc: 0.9406190777005685\n",
      "i_test_acc: 0.906\n",
      "\n",
      "epoch: 19 loss: 0.0020670934911454063 train_acc: 0.9767311600812969\n",
      "i_train_acc: 0.9426947026441657\n",
      "i_test_acc: 0.901\n",
      "\n",
      "epoch: 20 loss: 0.0019724342548997622 train_acc: 0.9772112599420679\n",
      "i_train_acc: 0.941972746142045\n",
      "i_test_acc: 0.906\n",
      "\n",
      "epoch: 21 loss: 0.0019374720507421832 train_acc: 0.9779794197193016\n",
      "i_train_acc: 0.9458983846223266\n",
      "i_test_acc: 0.901\n",
      "\n",
      "epoch: 22 loss: 0.001838485783517684 train_acc: 0.9780754396914558\n",
      "i_train_acc: 0.9482447432542189\n",
      "i_test_acc: 0.903\n",
      "\n",
      "epoch: 23 loss: 0.0018108503150845174 train_acc: 0.9788916094547666\n",
      "i_train_acc: 0.949733778539843\n",
      "i_test_acc: 0.898\n",
      "\n",
      "epoch: 24 loss: 0.0017248108856323825 train_acc: 0.9797877958615392\n",
      "i_train_acc: 0.9513130583882321\n",
      "i_test_acc: 0.899\n",
      "\n",
      "epoch: 25 loss: 0.00165682851685522 train_acc: 0.9809720421847744\n",
      "i_train_acc: 0.9519898926089703\n",
      "i_test_acc: 0.904\n",
      "\n",
      "epoch: 26 loss: 0.001646006085909918 train_acc: 0.9806199689535423\n",
      "i_train_acc: 0.9529825827993863\n",
      "i_test_acc: 0.909\n",
      "\n",
      "epoch: 27 loss: 0.0015906047663274373 train_acc: 0.9815481620176997\n",
      "i_train_acc: 0.9533435610504467\n",
      "i_test_acc: 0.902\n",
      "\n",
      "epoch: 28 loss: 0.0015265428084757125 train_acc: 0.98182021860547\n",
      "i_train_acc: 0.9561862647775472\n",
      "i_test_acc: 0.905\n",
      "\n",
      "epoch: 29 loss: 0.001517594515035073 train_acc: 0.9821722918367021\n",
      "i_train_acc: 0.957088710405198\n",
      "i_test_acc: 0.898\n",
      "\n",
      "epoch: 30 loss: 0.0014573575500518407 train_acc: 0.9827644149983197\n",
      "i_train_acc: 0.9585326234094396\n",
      "i_test_acc: 0.891\n",
      "\n",
      "epoch: 31 loss: 0.0014340011573469342 train_acc: 0.9829724582713204\n",
      "i_train_acc: 0.9580362783142315\n",
      "i_test_acc: 0.903\n",
      "\n",
      "epoch: 32 loss: 0.0014033937716133235 train_acc: 0.9831965048730136\n",
      "i_train_acc: 0.959389946755708\n",
      "i_test_acc: 0.905\n",
      "\n",
      "epoch: 33 loss: 0.0013882698345630883 train_acc: 0.9831004849008593\n",
      "i_train_acc: 0.961285082573775\n",
      "i_test_acc: 0.898\n",
      "\n",
      "epoch: 34 loss: 0.001367014371075556 train_acc: 0.9835485781042457\n",
      "i_train_acc: 0.960382636946124\n",
      "i_test_acc: 0.9\n",
      "\n",
      "epoch: 35 loss: 0.001325137068621225 train_acc: 0.9841407012658633\n",
      "i_train_acc: 0.9611948380110098\n",
      "i_test_acc: 0.892\n",
      "\n",
      "epoch: 36 loss: 0.0013357791078484713 train_acc: 0.9843647478675565\n",
      "i_train_acc: 0.9611948380110098\n",
      "i_test_acc: 0.907\n",
      "\n",
      "epoch: 37 loss: 0.0013022135059143009 train_acc: 0.9844287611823259\n",
      "i_train_acc: 0.9616009385434527\n",
      "i_test_acc: 0.899\n",
      "\n",
      "epoch: 38 loss: 0.001272814980467016 train_acc: 0.9843647478675565\n",
      "i_train_acc: 0.9633607075173721\n",
      "i_test_acc: 0.906\n",
      "\n",
      "epoch: 39 loss: 0.0012507348773558614 train_acc: 0.9848128410709428\n",
      "i_train_acc: 0.9621424059200433\n",
      "i_test_acc: 0.896\n",
      "\n",
      "epoch: 40 loss: 0.0012347321034746196 train_acc: 0.9849088610430969\n",
      "i_train_acc: 0.9628643624221641\n",
      "i_test_acc: 0.888\n",
      "\n",
      "epoch: 41 loss: 0.0012277314632843435 train_acc: 0.9848928577144046\n",
      "i_train_acc: 0.9627289955780164\n",
      "i_test_acc: 0.893\n",
      "\n",
      "epoch: 42 loss: 0.001198511214085255 train_acc: 0.9856290108342535\n",
      "i_train_acc: 0.9648948650843787\n",
      "i_test_acc: 0.901\n",
      "\n",
      "epoch: 43 loss: 0.0011672887544457963 train_acc: 0.985821050778562\n",
      "i_train_acc: 0.9633607075173721\n",
      "i_test_acc: 0.894\n",
      "\n",
      "epoch: 44 loss: 0.001145593059001111 train_acc: 0.9859330740794086\n",
      "i_train_acc: 0.9651204764912914\n",
      "i_test_acc: 0.894\n",
      "\n",
      "epoch: 45 loss: 0.0011557551957517235 train_acc: 0.9852129242882519\n",
      "i_train_acc: 0.9646692536774659\n",
      "i_test_acc: 0.894\n",
      "\n",
      "epoch: 46 loss: 0.0011503696847046298 train_acc: 0.9857410341351001\n",
      "i_train_acc: 0.9655265770237343\n",
      "i_test_acc: 0.897\n",
      "\n",
      "epoch: 47 loss: 0.0011273048763293548 train_acc: 0.986301150639333\n",
      "i_train_acc: 0.9640826640194928\n",
      "i_test_acc: 0.897\n",
      "\n",
      "epoch: 48 loss: 0.0011293918099535465 train_acc: 0.9860771040376398\n",
      "i_train_acc: 0.9660680444003249\n",
      "i_test_acc: 0.891\n",
      "\n",
      "epoch: 49 loss: 0.0011148232298980355 train_acc: 0.9861731240097941\n",
      "i_train_acc: 0.9665643894955329\n",
      "i_test_acc: 0.893\n",
      "\n",
      "epoch: 50 loss: 0.001086947386966025 train_acc: 0.9866372205418728\n",
      "i_train_acc: 0.9660229221189424\n",
      "i_test_acc: 0.9\n",
      "\n",
      "epoch: 51 loss: 0.0010900338240233595 train_acc: 0.9863651639541025\n",
      "i_train_acc: 0.9679631802183919\n",
      "i_test_acc: 0.897\n",
      "\n",
      "epoch: 52 loss: 0.0010844415961037052 train_acc: 0.9861251140237169\n",
      "i_train_acc: 0.9657973107120296\n",
      "i_test_acc: 0.897\n",
      "\n",
      "epoch: 53 loss: 0.0010646065708445889 train_acc: 0.9865732072271033\n",
      "i_train_acc: 0.967557079685949\n",
      "i_test_acc: 0.901\n",
      "\n",
      "epoch: 54 loss: 0.0010680223142778347 train_acc: 0.9865732072271033\n",
      "i_train_acc: 0.9668351231838281\n",
      "i_test_acc: 0.907\n",
      "\n",
      "epoch: 55 loss: 0.001068363843236007 train_acc: 0.9867332405140269\n",
      "i_train_acc: 0.9662936558072376\n",
      "i_test_acc: 0.897\n",
      "\n",
      "epoch: 56 loss: 0.0010675699129410446 train_acc: 0.9863651639541025\n",
      "i_train_acc: 0.967647324248714\n",
      "i_test_acc: 0.897\n",
      "\n",
      "epoch: 57 loss: 0.0010315303050336404 train_acc: 0.9866532238705651\n",
      "i_train_acc: 0.9672863459976536\n",
      "i_test_acc: 0.898\n",
      "\n",
      "epoch: 58 loss: 0.0010376048618660216 train_acc: 0.9867972538287964\n",
      "i_train_acc: 0.9692266040971032\n",
      "i_test_acc: 0.896\n",
      "\n",
      "epoch: 59 loss: 0.0010290927414202927 train_acc: 0.9868292604861811\n",
      "i_train_acc: 0.9684144030322173\n",
      "i_test_acc: 0.901\n",
      "\n",
      "epoch: 60 loss: 0.0010366001631133997 train_acc: 0.9869732904444124\n",
      "i_train_acc: 0.9683241584694522\n",
      "i_test_acc: 0.9\n",
      "\n",
      "epoch: 61 loss: 0.001029146715039538 train_acc: 0.9868772704722583\n",
      "i_train_acc: 0.9683241584694522\n",
      "i_test_acc: 0.9\n",
      "\n",
      "epoch: 62 loss: 0.001027852071923921 train_acc: 0.9870693104165666\n",
      "i_train_acc: 0.9679631802183919\n",
      "i_test_acc: 0.892\n",
      "\n",
      "epoch: 63 loss: 0.0010231912145333428 train_acc: 0.9867972538287964\n",
      "i_train_acc: 0.9688205035646602\n",
      "i_test_acc: 0.896\n",
      "\n",
      "epoch: 64 loss: 0.0009983070215880616 train_acc: 0.9870533070878743\n",
      "i_train_acc: 0.9694070932226333\n",
      "i_test_acc: 0.896\n",
      "\n",
      "epoch: 65 loss: 0.0010010072185265503 train_acc: 0.9870373037591819\n",
      "i_train_acc: 0.9686400144391301\n",
      "i_test_acc: 0.901\n",
      "\n",
      "epoch: 66 loss: 0.0010126793029153185 train_acc: 0.9871493270600284\n",
      "i_train_acc: 0.9689558704088078\n",
      "i_test_acc: 0.896\n",
      "\n",
      "epoch: 67 loss: 0.0010042491102409056 train_acc: 0.9870373037591819\n",
      "i_train_acc: 0.969046114971573\n",
      "i_test_acc: 0.9\n",
      "\n",
      "epoch: 68 loss: 0.0009859578317319147 train_acc: 0.9873253636756445\n",
      "i_train_acc: 0.9698131937550762\n",
      "i_test_acc: 0.901\n",
      "\n",
      "epoch: 69 loss: 0.0009890256714071212 train_acc: 0.9871813337174132\n",
      "i_train_acc: 0.9687753812832777\n",
      "i_test_acc: 0.903\n",
      "\n",
      "epoch: 70 loss: 0.0009874221472173385 train_acc: 0.987341367004337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_train_acc: 0.9694070932226333\n",
      "i_test_acc: 0.903\n",
      "\n",
      "epoch: 71 loss: 0.000985603868815988 train_acc: 0.9874533903051835\n",
      "i_train_acc: 0.9697229491923112\n",
      "i_test_acc: 0.902\n",
      "\n",
      "epoch: 72 loss: 0.0009632373882967169 train_acc: 0.9876294269207996\n",
      "i_train_acc: 0.9701290497247541\n",
      "i_test_acc: 0.897\n",
      "\n",
      "epoch: 73 loss: 0.0009624543700835266 train_acc: 0.9877094435642614\n",
      "i_train_acc: 0.9702192942875192\n",
      "i_test_acc: 0.898\n",
      "\n",
      "epoch: 74 loss: 0.0009458808051188875 train_acc: 0.9879494934946469\n",
      "i_train_acc: 0.969046114971573\n",
      "i_test_acc: 0.902\n",
      "\n",
      "epoch: 75 loss: 0.0009655061252713861 train_acc: 0.9877734568790308\n",
      "i_train_acc: 0.9696327046295461\n",
      "i_test_acc: 0.899\n",
      "\n",
      "epoch: 76 loss: 0.0009563460167264622 train_acc: 0.987821466865108\n",
      "i_train_acc: 0.969046114971573\n",
      "i_test_acc: 0.891\n",
      "\n",
      "epoch: 77 loss: 0.0009382460607752435 train_acc: 0.9879014835085698\n",
      "i_train_acc: 0.9703997834130493\n",
      "i_test_acc: 0.899\n",
      "\n",
      "epoch: 78 loss: 0.0009453238196207665 train_acc: 0.9877414502216461\n",
      "i_train_acc: 0.9704449056944319\n",
      "i_test_acc: 0.903\n",
      "\n",
      "epoch: 79 loss: 0.0009623211463133679 train_acc: 0.987693440235569\n",
      "i_train_acc: 0.9707156393827272\n",
      "i_test_acc: 0.902\n",
      "\n",
      "epoch: 80 loss: 0.0009208572849826988 train_acc: 0.9878054635364156\n",
      "i_train_acc: 0.9707156393827272\n",
      "i_test_acc: 0.898\n",
      "\n",
      "epoch: 81 loss: 0.000926160988647647 train_acc: 0.9879494934946469\n",
      "i_train_acc: 0.9696327046295461\n",
      "i_test_acc: 0.905\n",
      "\n",
      "epoch: 82 loss: 0.000954846666171777 train_acc: 0.987693440235569\n",
      "i_train_acc: 0.9699485605992239\n",
      "i_test_acc: 0.91\n",
      "\n",
      "epoch: 83 loss: 0.0009333168940841393 train_acc: 0.9876614335781843\n",
      "i_train_acc: 0.9706705171013447\n",
      "i_test_acc: 0.899\n",
      "\n",
      "epoch: 84 loss: 0.0009418232621221711 train_acc: 0.9879014835085698\n",
      "i_train_acc: 0.9709412507896399\n",
      "i_test_acc: 0.903\n",
      "\n",
      "epoch: 85 loss: 0.0009252305891724129 train_acc: 0.9880935234528782\n",
      "i_train_acc: 0.971527840447613\n",
      "i_test_acc: 0.905\n",
      "\n",
      "epoch: 86 loss: 0.0009178667546730089 train_acc: 0.987821466865108\n",
      "i_train_acc: 0.9711668621965527\n",
      "i_test_acc: 0.909\n",
      "\n",
      "epoch: 87 loss: 0.0009139592300245048 train_acc: 0.9879815001520317\n",
      "i_train_acc: 0.9709863730710224\n",
      "i_test_acc: 0.904\n",
      "\n",
      "epoch: 88 loss: 0.000932988330471404 train_acc: 0.9877414502216461\n",
      "i_train_acc: 0.9703546611316668\n",
      "i_test_acc: 0.904\n",
      "\n",
      "epoch: 89 loss: 0.0008995197038733705 train_acc: 0.9880935234528782\n",
      "i_train_acc: 0.9707607616641097\n",
      "i_test_acc: 0.901\n",
      "\n",
      "epoch: 90 loss: 0.0009223361648307378 train_acc: 0.9877254468929537\n",
      "i_train_acc: 0.9704900279758144\n",
      "i_test_acc: 0.904\n",
      "\n",
      "epoch: 91 loss: 0.0008974197404921822 train_acc: 0.9880935234528782\n",
      "i_train_acc: 0.970038805161989\n",
      "i_test_acc: 0.905\n",
      "\n",
      "epoch: 92 loss: 0.0009095879561484546 train_acc: 0.9879494934946469\n",
      "i_train_acc: 0.9699485605992239\n",
      "i_test_acc: 0.898\n",
      "\n",
      "epoch: 93 loss: 0.0009159969631806062 train_acc: 0.9877094435642614\n",
      "i_train_acc: 0.9699936828806064\n",
      "i_test_acc: 0.903\n",
      "\n",
      "epoch: 94 loss: 0.0009219430728769233 train_acc: 0.987693440235569\n",
      "i_train_acc: 0.9709412507896399\n",
      "i_test_acc: 0.899\n",
      "\n",
      "epoch: 95 loss: 0.0009148557141221884 train_acc: 0.9879334901659546\n",
      "i_train_acc: 0.9710766176337876\n",
      "i_test_acc: 0.907\n",
      "\n",
      "epoch: 96 loss: 0.0009059697530492643 train_acc: 0.9881575367676477\n",
      "i_train_acc: 0.9712571067593178\n",
      "i_test_acc: 0.905\n",
      "\n",
      "epoch: 97 loss: 0.0008891074576187519 train_acc: 0.9879014835085698\n",
      "i_train_acc: 0.9712571067593178\n",
      "i_test_acc: 0.905\n",
      "\n",
      "epoch: 98 loss: 0.0009067054234271103 train_acc: 0.9877574535503385\n",
      "i_train_acc: 0.9706705171013447\n",
      "i_test_acc: 0.907\n",
      "\n",
      "epoch: 99 loss: 0.0009079121162891849 train_acc: 0.9879494934946469\n",
      "i_train_acc: 0.9706705171013447\n",
      "i_test_acc: 0.908\n",
      "\n",
      "epoch: 100 loss: 0.0008854680950739605 train_acc: 0.9883175700545713\n",
      "i_train_acc: 0.9710766176337876\n",
      "i_test_acc: 0.903\n",
      "\n",
      "epoch: 101 loss: 0.0008991292333909489 train_acc: 0.9881255301102629\n",
      "i_train_acc: 0.970535150257197\n",
      "i_test_acc: 0.903\n",
      "\n",
      "epoch: 102 loss: 0.0008901352807753367 train_acc: 0.9882375534111095\n",
      "i_train_acc: 0.9716180850103782\n",
      "i_test_acc: 0.899\n",
      "\n",
      "epoch: 103 loss: 0.0008891517866565224 train_acc: 0.9880295101381087\n",
      "i_train_acc: 0.9713473513220828\n",
      "i_test_acc: 0.898\n",
      "\n",
      "epoch: 104 loss: 0.0008918010733920404 train_acc: 0.9879334901659546\n",
      "i_train_acc: 0.9704449056944319\n",
      "i_test_acc: 0.897\n",
      "\n",
      "epoch: 105 loss: 0.0008777661991498265 train_acc: 0.9880615167954935\n",
      "i_train_acc: 0.9705802725385796\n",
      "i_test_acc: 0.908\n",
      "\n",
      "epoch: 106 loss: 0.0008988361500494034 train_acc: 0.9880775201241858\n",
      "i_train_acc: 0.9710314953524051\n",
      "i_test_acc: 0.906\n",
      "\n",
      "epoch: 107 loss: 0.0008886117044357942 train_acc: 0.9882855633971867\n",
      "i_train_acc: 0.9711217399151701\n",
      "i_test_acc: 0.904\n",
      "\n",
      "epoch: 108 loss: 0.0008923119140052247 train_acc: 0.9880295101381087\n",
      "i_train_acc: 0.9712571067593178\n",
      "i_test_acc: 0.899\n",
      "\n",
      "epoch: 109 loss: 0.0008796908221321234 train_acc: 0.9883335733832637\n",
      "i_train_acc: 0.9714827181662304\n",
      "i_test_acc: 0.899\n",
      "\n",
      "epoch: 110 loss: 0.0008795829849094374 train_acc: 0.9879815001520317\n",
      "i_train_acc: 0.9712571067593178\n",
      "i_test_acc: 0.904\n",
      "\n",
      "epoch: 111 loss: 0.0008733928521520465 train_acc: 0.988605629971034\n",
      "i_train_acc: 0.9709412507896399\n",
      "i_test_acc: 0.904\n",
      "\n",
      "epoch: 112 loss: 0.0008672046413475866 train_acc: 0.9883335733832637\n",
      "i_train_acc: 0.9715729627289956\n",
      "i_test_acc: 0.9\n",
      "\n",
      "epoch: 113 loss: 0.0008748619371599034 train_acc: 0.9882055467537247\n",
      "i_train_acc: 0.9718436964172908\n",
      "i_test_acc: 0.907\n",
      "\n",
      "epoch: 114 loss: 0.0008653912186129706 train_acc: 0.9884135900267256\n",
      "i_train_acc: 0.9713924736034654\n",
      "i_test_acc: 0.903\n",
      "\n",
      "epoch: 115 loss: 0.0008704192015650782 train_acc: 0.9883975866980332\n",
      "i_train_acc: 0.9712119844779352\n",
      "i_test_acc: 0.894\n",
      "\n",
      "epoch: 116 loss: 0.0008709830282575733 train_acc: 0.9881735400963401\n",
      "i_train_acc: 0.9711668621965527\n",
      "i_test_acc: 0.898\n",
      "\n",
      "epoch: 117 loss: 0.00086533755007557 train_acc: 0.9884455966841103\n",
      "i_train_acc: 0.9709412507896399\n",
      "i_test_acc: 0.898\n",
      "\n",
      "epoch: 118 loss: 0.0008809136429083705 train_acc: 0.9882695600684942\n",
      "i_train_acc: 0.9710766176337876\n",
      "i_test_acc: 0.906\n",
      "\n",
      "epoch: 119 loss: 0.0008695448427148321 train_acc: 0.9882855633971867\n",
      "i_train_acc: 0.9712571067593178\n",
      "i_test_acc: 0.906\n",
      "\n",
      "epoch: 120 loss: 0.0008704843122530225 train_acc: 0.9883655800406485\n",
      "i_train_acc: 0.9708058839454923\n",
      "i_test_acc: 0.899\n",
      "\n",
      "epoch: 121 loss: 0.0008723589735325501 train_acc: 0.9881415334389553\n",
      "i_train_acc: 0.9716632072917607\n",
      "i_test_acc: 0.908\n",
      "\n",
      "epoch: 122 loss: 0.0008648960425965994 train_acc: 0.9883815833693408\n",
      "i_train_acc: 0.9721595523869687\n",
      "i_test_acc: 0.903\n",
      "\n",
      "epoch: 123 loss: 0.000849813604745347 train_acc: 0.9884936066701874\n",
      "i_train_acc: 0.9710314953524051\n",
      "i_test_acc: 0.901\n",
      "\n",
      "epoch: 124 loss: 0.0008648533121440166 train_acc: 0.988301566725879\n",
      "i_train_acc: 0.9709412507896399\n",
      "i_test_acc: 0.902\n",
      "\n",
      "epoch: 125 loss: 0.0008732508698144993 train_acc: 0.9882215500824172\n",
      "i_train_acc: 0.9712571067593178\n",
      "i_test_acc: 0.9\n",
      "\n",
      "epoch: 126 loss: 0.0008633656404594832 train_acc: 0.9885736233136493\n",
      "i_train_acc: 0.9717985741359083\n",
      "i_test_acc: 0.901\n",
      "\n",
      "epoch: 127 loss: 0.0008662072601992766 train_acc: 0.9882215500824172\n",
      "i_train_acc: 0.9719790632614385\n",
      "i_test_acc: 0.898\n",
      "\n",
      "epoch: 128 loss: 0.0008599446183374438 train_acc: 0.9882535567398019\n",
      "i_train_acc: 0.972024185542821\n",
      "i_test_acc: 0.903\n",
      "\n",
      "epoch: 129 loss: 0.0008433235183883777 train_acc: 0.98878166658665\n",
      "i_train_acc: 0.9715729627289956\n",
      "i_test_acc: 0.906\n",
      "\n",
      "epoch: 130 loss: 0.000849410950004178 train_acc: 0.9883335733832637\n",
      "i_train_acc: 0.9708961285082574\n",
      "i_test_acc: 0.906\n",
      "\n",
      "epoch: 131 loss: 0.0008610177225083032 train_acc: 0.9882055467537247\n",
      "i_train_acc: 0.9716632072917607\n",
      "i_test_acc: 0.9\n",
      "\n",
      "epoch: 132 loss: 0.0008556852391989786 train_acc: 0.9885896266423416\n",
      "i_train_acc: 0.9714375958848479\n",
      "i_test_acc: 0.912\n",
      "\n",
      "epoch: 133 loss: 0.0008611213214239077 train_acc: 0.9883335733832637\n",
      "i_train_acc: 0.9712571067593178\n",
      "i_test_acc: 0.904\n",
      "\n",
      "epoch: 134 loss: 0.000844934816169497 train_acc: 0.9885256133275722\n",
      "i_train_acc: 0.9717083295731432\n",
      "i_test_acc: 0.904\n",
      "\n",
      "epoch: 135 loss: 0.0008399568843227878 train_acc: 0.9886376366284187\n",
      "i_train_acc: 0.9712571067593178\n",
      "i_test_acc: 0.906\n",
      "\n",
      "epoch: 136 loss: 0.0008431358244855163 train_acc: 0.9887496599292653\n",
      "i_train_acc: 0.9717985741359083\n",
      "i_test_acc: 0.902\n",
      "\n",
      "epoch: 137 loss: 0.0008555372902432869 train_acc: 0.9885576199849568\n",
      "i_train_acc: 0.971527840447613\n",
      "i_test_acc: 0.904\n",
      "\n",
      "epoch: 138 loss: 0.0008444275894889572 train_acc: 0.9885576199849568\n",
      "i_train_acc: 0.9716180850103782\n",
      "i_test_acc: 0.908\n",
      "\n",
      "epoch: 139 loss: 0.0008315233135738023 train_acc: 0.9887016499431882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_train_acc: 0.9713473513220828\n",
      "i_test_acc: 0.901\n",
      "\n",
      "epoch: 140 loss: 0.0008409299748511901 train_acc: 0.9886376366284187\n",
      "i_train_acc: 0.9714827181662304\n",
      "i_test_acc: 0.902\n",
      "\n",
      "epoch: 141 loss: 0.0008471011875249769 train_acc: 0.988605629971034\n",
      "i_train_acc: 0.9717985741359083\n",
      "i_test_acc: 0.896\n",
      "\n",
      "epoch: 142 loss: 0.0008505402256498916 train_acc: 0.9884295933554179\n",
      "i_train_acc: 0.9718888186986734\n",
      "i_test_acc: 0.9\n",
      "\n",
      "epoch: 143 loss: 0.0008474116891236942 train_acc: 0.9883495767119561\n",
      "i_train_acc: 0.9724302860752639\n",
      "i_test_acc: 0.896\n",
      "\n",
      "epoch: 144 loss: 0.000840722400811974 train_acc: 0.9885576199849568\n",
      "i_train_acc: 0.9716632072917607\n",
      "i_test_acc: 0.904\n",
      "\n",
      "epoch: 145 loss: 0.0008475731486977455 train_acc: 0.9883335733832637\n",
      "i_train_acc: 0.9716632072917607\n",
      "i_test_acc: 0.906\n",
      "\n",
      "epoch: 146 loss: 0.0008337691031302826 train_acc: 0.9889416998735737\n",
      "i_train_acc: 0.9714827181662304\n",
      "i_test_acc: 0.909\n",
      "\n",
      "epoch: 147 loss: 0.0008408823107152624 train_acc: 0.9886376366284187\n",
      "i_train_acc: 0.9719339409800559\n",
      "i_test_acc: 0.901\n",
      "\n",
      "epoch: 148 loss: 0.0008366818496710041 train_acc: 0.9885896266423416\n",
      "i_train_acc: 0.9718888186986734\n",
      "i_test_acc: 0.906\n",
      "\n",
      "epoch: 149 loss: 0.0008373937930256647 train_acc: 0.9884616000128027\n",
      "i_train_acc: 0.9722497969497338\n",
      "i_test_acc: 0.906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\"batch_size\": 32,\n",
    "          \"start_epoch\" : 1,\n",
    "          \"n_epoch\": 150,\n",
    "          \"rnn_classes\":5,\n",
    "          \"save_file\": \"model_xxx_att.pkl\"\n",
    "         }\n",
    "encode_params = {\n",
    "    \"emb_size\":  128,\n",
    "    \"hidden_size\": 128,\n",
    "    \"input_dropout_p\": 0.2,\n",
    "    \"dropout_p\": 0.3,\n",
    "    \"n_layers\": 1,\n",
    "    \"bidirectional\": True,\n",
    "    \"rnn_cell\": None,\n",
    "    \"rnn_cell_name\": 'lstm',\n",
    "    \"variable_lengths_flag\": True\n",
    "}\n",
    "\n",
    "sni_net = SNI_net(data_loader, encode_params).cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, sni_net.parameters()), \\\n",
    "                            lr=0.01, momentum=0.9, dampening=0.0)\n",
    "\n",
    "trainer = Trainer(data_loader, params, optimizer)\n",
    "trainer.train(sni_net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
